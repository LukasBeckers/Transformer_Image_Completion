{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LukasBeckers/Transformer_Image_Completion/blob/main/Transformer_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "CxC4bo9MAjle"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tensorflow.keras import layers\n",
        "from glob import glob, escape\n",
        "from keras.layers import activation\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pickle as pk\n",
        "import numpy as np\n",
        "import shutil\n",
        "import random\n",
        "import time\n",
        "import cv2\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qZAYc0bAjlg",
        "outputId": "2e527161-33e0-4bdb-c188-94b7b5ee2664"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 7299745093946243772\n",
            "xla_global_id: -1\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14417788928\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 14210084943639645754\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "xla_global_id: 416903419\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# Checking if a GPU is available on the system.\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "a50f2ec9b89c402689944324118e811e",
            "6cb2c3cfdcac490f922130c08c7ab0e4",
            "fc227b4eb2c34a29977e01452218d0e9",
            "717fa8ef14c247c48d3f8f7cdc59192a",
            "59f43cd43cb7482e9c218aae94e64388",
            "802104c207e34202927da2a56e4a66c5",
            "d00ead86656c416e983b7908bdd37cc2",
            "2848dca4e71e45a4884facb0021c2058",
            "ad79248fc11e4c439026e4591030a44d",
            "dc2f769afa1f4378b6dc60a2d70c7765",
            "3cf0096cd0e2455fb8d521ca059747c6"
          ]
        },
        "id": "95BvwlskRIns",
        "cellView": "form",
        "outputId": "7101f77e-325b-41c0-bade-570cb6dad4c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...:   0%|          | 0/5 [00:00<?, ? file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a50f2ec9b89c402689944324118e811e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\n",
            "\n",
            "Number of images in the train-subset of the MNIST-dataset: 60000\n",
            "Number of images in the test-subset of the MNIST-dataset: 10000\n"
          ]
        }
      ],
      "source": [
        "#@markdown # Download the MNIST Dataset\n",
        "#@markdown The MNIST-dataset was chosen for this project, because my computational recources are very limited and the classical MNIST-dataset is not very computationally intensive.\n",
        "batch_size = 500 #@param {type:\"integer\"}\n",
        "\n",
        "mnist = tfds.load('MNIST', batch_size=batch_size)\n",
        "\n",
        "train_ds = mnist['train']\n",
        "test_ds = mnist['test']\n",
        "print(f'\\nNumber of images in the train-subset of the MNIST-dataset: {len(train_ds)*batch_size}')\n",
        "print(f'Number of images in the test-subset of the MNIST-dataset: {len(test_ds)*batch_size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "Mj441_90lZrn",
        "outputId": "5fbc4482-7f03-4cb6-dab5-1abc71eabd9d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Test Image')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Testing and Debugging\n",
        "\n",
        "for test_batch in train_ds:\n",
        "    break\n",
        "\n",
        "test_images = test_batch['image']\n",
        "test_image =  test_images[0]\n",
        "\n",
        "plt.imshow(test_image[:,:,0], cmap='gray')\n",
        "plt.title('Test Image')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcrBSxkokEA0"
      },
      "source": [
        "For this project, I decided to code a transformer model from scratch. \n",
        "\n",
        "A transformer-network handles each element of a sequence individually.\n",
        "A relation between the elements is created in a transformer network via multihead-self-attention.\n",
        "For this reason, each image must be converted to a sequence of vectors. \n",
        "This is done by splitting the images into smaller segments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dZrwG-KmDRQs"
      },
      "outputs": [],
      "source": [
        "def split_image(image, segment_width):\n",
        "    '''\n",
        "    This function crops the image, so that the width and the height of the image \n",
        "    are divisible by the 'segment_width' parameter. \n",
        "    The cropped image is split into square-segments in the size of the 'segment_width'.\n",
        "\n",
        "    :param image:           image (tf-tensor or np-array) of shape[heigth, width, 1]\n",
        "    :param segment_width:   Width of the segments-squares that the image will be split into\n",
        "\n",
        "    :return:                tf-tensor of shape[height//segment_width, width//segment_width, segment_width, segment_width, 1].\n",
        "                            If the image width or height is not divisible by the segment_width, it will be cropped.\n",
        "    '''\n",
        "    # Cropping the image\n",
        "    image = image[:image.shape[0] // segment_width * segment_width]   # Cropping height\n",
        "    image = image[:,:image.shape[1] // segment_width * segment_width] # Cropping width\n",
        "    # Splitting the image.\n",
        "    # First the width is split, then the height of the image is split.\n",
        "    image = tf.reshape(image, [image.shape[0],                        # height\n",
        "                               image.shape[1]//segment_width,         # N_width_segments\n",
        "                               segment_width,                         # segment_width\n",
        "                               image.shape[2]                         # 1\n",
        "                               ])\n",
        "    image = tf.transpose(image, [1, 0, 2, 3])                         # Swapping height and N_width_segments\n",
        "    image = tf.reshape(image, [image.shape[0],                        # N_width_segments\n",
        "                               image.shape[1]//segment_width,         # N_height_segments\n",
        "                               segment_width,                         # segment_width\n",
        "                               segment_width,                         # segment_width\n",
        "                               image.shape[3]                         # 1\n",
        "                               ])\n",
        "    image = tf.transpose(image, [1, 0, 2, 3, 4])                      # Swapping N_width_segments and N_height_segments\n",
        "    # Output shape: [N_height_segments, N_width_segments, segment_width, segment_width, 1]\n",
        "    return image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1km8PxVgl8gQ"
      },
      "source": [
        "The output of the transformer-model will also have the shape of a split-image.\n",
        "For this reason, a function to reverse the split process is needed.\n",
        "This function will be able to create normal (non-split) images from split-images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qQ8QZUvKl9Ti"
      },
      "outputs": [],
      "source": [
        "def unsplit_image(image):\n",
        "    '''\n",
        "    This function reverses the split_image function (except the cropping).\n",
        "    \n",
        "    :param image:           split RGB-image tf-tensor of shape[N_height_segments, N_width_segments, segment_width, segment_width, RGB]\n",
        "\n",
        "    :return:                RGB-image-batch tf-tensor of shape[N_images, heigth, width, RGB]\n",
        "    '''\n",
        "    image = tf.transpose(image, [1, 0, 2, 3, 4])                      # Swapping N_width_segments and N_height_segments\n",
        "    image = tf.reshape(image, [image.shape[0],                        # N_width_segments\n",
        "                               image.shape[1]*image.shape[2],         # N_height_segments * segment_width = height\n",
        "                               image.shape[3],                        # segment_width\n",
        "                               image.shape[4]                         # RGB\n",
        "                               ])\n",
        "    image = tf.transpose(image, [1, 0, 2, 3])                         # Swapping height and N_width_segments\n",
        "    image = tf.reshape(image, [image.shape[0],                        # height\n",
        "                               image.shape[1]*image.shape[2],         # N_width_segments * segment_width = width\n",
        "                               image.shape[3]                         # RGB\n",
        "                               ])\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "wIiOIs9ZFOSu",
        "outputId": "13dda8a5-040d-4654-a096-62d0a64dfdab"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 504x504 with 49 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Testing and Debugging\n",
        "\n",
        "# Splitting the image.\n",
        "\n",
        "segment_width = 4\n",
        "split_test_image = split_image(test_image, segment_width)\n",
        "\n",
        "\n",
        "# Plotting the split-test-image.\n",
        "ig, axs = plt.subplots(test_image.shape[0]//segment_width,\n",
        "                           test_image.shape[1]//segment_width,\n",
        "                           figsize=[7, 7])\n",
        "\n",
        "for i_row_g, row_g  in enumerate(axs):\n",
        "    for i_col_g, col_g in enumerate(row_g):\n",
        "        col_g.imshow(split_test_image[i_row_g, i_col_g][:,:,0], cmap='gray')\n",
        "\n",
        "        # Removing the numbers on the axes.\n",
        "        col_g.set_xticks([])\n",
        "        col_g.set_yticks([])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "2dnY9MXZqNew",
        "outputId": "b2448ad7-826c-4347-9ac9-fec8259e83f0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Testing and Debugging\n",
        "\n",
        "# Unsplitting the test-image.\n",
        "unsplit_test_image = unsplit_image(split_test_image)\n",
        "\n",
        "# Plotting the unsplit test-image.\n",
        "plt.imshow(unsplit_test_image[:,:,0], cmap='gray')\n",
        "plt.title('Unsplit Image')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZfhJ3UIm7lM"
      },
      "source": [
        "For the training-process, a version of the split image function is needed,\n",
        "that can split batches of images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PonClPAJqhLU"
      },
      "outputs": [],
      "source": [
        "def split_image_batch(batch, segment_width):\n",
        "    \"\"\"\n",
        "    A version of the split_image function that can be applied to batches of images.\n",
        "\n",
        "    :param batch:   tf.tensor: Batch of images, shape=[batch_size, height, width, RGB]\n",
        "    :param segment_width: int: Width of the segments-squares that the image will be split into.\n",
        "\n",
        "    :return:        tf-tensor: Batch of split images, shape = [batch_size, \n",
        "                                                               height//segment_width, \n",
        "                                                               width//segment_width, \n",
        "                                                               segment_width,\n",
        "                                                               segment_width,\n",
        "                                                               RGB].\n",
        "                               If the image width or height is not divisible by the segment_width, it will be cropped.\n",
        "    \"\"\"\n",
        " \n",
        "    # Cropping the images.\n",
        "    batch = batch[:,:batch.shape[1] // segment_width * segment_width]   # Cropping height\n",
        "    batch = batch[:,:,:batch.shape[2] // segment_width * segment_width] # Cropping width\n",
        "    # Splitting the images.\n",
        "    # First the width is split, then the height of the image is split.\n",
        "    batch = tf.reshape(batch, [batch.shape[0],                          # batch-size\n",
        "                               batch.shape[1],                          # height\n",
        "                               batch.shape[2]//segment_width,           # N_width_segments\n",
        "                               segment_width,                           # segment_width\n",
        "                               batch.shape[3]                           # RGB\n",
        "                               ])\n",
        "    batch = tf.transpose(batch, [0, 2, 1, 3, 4])                        # Swapping height and N_width_segments\n",
        "    batch = tf.reshape(batch, [batch.shape[0],                          # batch-size\n",
        "                               batch.shape[1],                          # N_width_segments\n",
        "                               batch.shape[2]//segment_width,           # N_height_segments\n",
        "                               segment_width,                           # segment_width\n",
        "                               segment_width,                           # segment_width\n",
        "                               batch.shape[4]                           # RGB\n",
        "                               ])\n",
        "    batch = tf.transpose(batch, [0, 2, 1, 3, 4, 5])                     # Swapping N_width_segments and N_height_segments\n",
        "    # Output shape: [batch_size, N_height_segments, N_width_segments, segment_width, segment_width, RGB]\n",
        "    return batch\n",
        "\n",
        "\n",
        "def unsplit_batch(batch):\n",
        "    '''\n",
        "    This reverses the 'split_image_batch' function, excep for the cropping.\n",
        "    It will be used to unsplit the output of the model.\n",
        "    :param batch:           split-images batch tf-tensor of shape[N_images,\n",
        "                                                                  N_height_segments,\n",
        "                                                                  N_width_segments,\n",
        "                                                                  segment_width,\n",
        "                                                                  segment_width, \n",
        "                                                                  RGB]\n",
        "    :return:                RGB-image batch tf-tensor of shape[N_images, height, width, RGB]\n",
        "    '''\n",
        "    batch = tf.transpose(batch, [0, 2, 1, 3, 4])                           # Swapping N_width_segments and N_height_segments\n",
        "    batch = tf.reshape(batch, [-1,                                         # N_images\n",
        "                               tf.shape(batch)[1],                         # N_width_segments\n",
        "                               tf.shape(batch)[2]*tf.shape(batch)[3],      # N_height_segments * segment_width = height\n",
        "                               tf.shape(batch)[4],                         # segment_width\n",
        "                               ])  \t                                   \n",
        "    batch = tf.transpose(batch, [0, 2, 1, 3])                              # Swapping height and N_width_segments\n",
        "    batch = tf.reshape(batch, [-1,                                         # N_images\n",
        "                               tf.shape(batch)[1],                         # height\n",
        "                               tf.shape(batch)[2]*tf.shape(batch)[3],      # N_width_segments * segment_width = width\n",
        "                               ])\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "w-COT_-Mgilp"
      },
      "outputs": [],
      "source": [
        "def flatten_batch(batch):\n",
        "    '''\n",
        "    This function can be used inside the call-method of a keras model.\n",
        "    This function will be used to flatten the image-batches for the transformer-layers in the model.\n",
        "    :param images: (tf.tensor): Batch of split-images.\n",
        "                                Shape for batch of images: [batch_size, n_height_segments, n_width_segments, segment_width, segment_width, RGB]\n",
        "\n",
        "    :return:       (tf.tensor): flattend batch of images.\n",
        "                                Shape: [batch_size, n_height_segments * n_width_segments, ?]\n",
        "    '''\n",
        "    out = tf.reshape(batch, [-1, tf.shape(batch)[1]*tf.shape(batch)[2], tf.shape(batch)[3]*tf.shape(batch)[4]*tf.shape(batch)[5]])\n",
        "    return out\n",
        "\n",
        "def unflatten_batch(images, n_height_segments, n_width_segments, segment_width):\n",
        "    '''\n",
        "    This function can be used inside the call-method of a keras model.\n",
        "    This function reverses the flatten operation done to a batch of images by the 'flatten_batch' function\n",
        "    :param images: (tf.tensor): Batch of flattend RGB-images.\n",
        "                                Shape: [batch_size, n_height_segments * n_width_segments, ?]\n",
        "\n",
        "    :return:       (tf.tensor): Batch of split RGB-images.\n",
        "                                Shape: [batch_size, n_height_segments, n_width_segments, segment_width, segment_width, RGB]\n",
        "    '''\n",
        "    out = tf.reshape(images, [-1, n_height_segments, n_width_segments, segment_width, segment_width])\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0klIAc04f6G"
      },
      "source": [
        "Now the split_image and the flatten_batch operations are performed on the datasets, \n",
        "also the last image segment is dropped from the sequence, because it should be predicted by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qKHGSdGDXdCr"
      },
      "outputs": [],
      "source": [
        "def transform_dataset(dataset, segment_width):\n",
        "    def extract_images(batch):\n",
        "        batch = tf.cast(batch['image'], tf.float32)\n",
        "        # Normalizing between 0 and 1\n",
        "        batch /= 255\n",
        "        return batch\n",
        "\n",
        "    def split_image_wrapper(batch):\n",
        "        image = tf.numpy_function(split_image_batch, [batch, segment_width], tf.float32)\n",
        "        return image\n",
        "\n",
        "    def remove_last_element(batch):\n",
        "        \"\"\"\n",
        "        The model will be trained to predict the next image segment of the image\n",
        "        autoregressively. For this reason the last element of the split- and  \n",
        "        flattend-images is removed. \n",
        "        In the call function of the model, a start token will be added to the \n",
        "        start of the image-sequence. This way, the input and target sequence are \n",
        "        shifted for on image-segment to each other.\n",
        "        \"\"\"\n",
        "        inputs = batch[:,:-1]\n",
        "        targets = batch\n",
        "        return inputs, targets\n",
        "    dataset = dataset.map(extract_images)\n",
        "    dataset = dataset.map(split_image_wrapper)\n",
        "    dataset = dataset.map(flatten_batch)\n",
        "    dataset = dataset.map(remove_last_element)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ifryjwz7Z-zK",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # Processing the Datasets\n",
        "\n",
        "segment_width = 4 #@param {type:\"integer\"}\n",
        "train_ds_processed = transform_dataset(train_ds, segment_width)\n",
        "test_ds_processed = transform_dataset(test_ds, segment_width)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpjSegCqqXjo",
        "outputId": "ad962edc-f862-4c20-836b-28eacd4567d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 48, 16) (500, 49, 16)\n"
          ]
        }
      ],
      "source": [
        "# Testing and Debugging\n",
        "\n",
        "for test_batch in train_ds_processed:\n",
        "    break\n",
        "\n",
        "print(test_batch[0].shape, test_batch[1].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0vku4TzZXql"
      },
      "source": [
        "The envisioned transformer-model will be a decoder only model, which will be \n",
        "trained autoregressively. \n",
        "For this reason a forward facing attention mask will be created in the \n",
        "MultiHeadSelfAttention foreward call. \n",
        "This mask will prevent the transformer-model from attending to image-semgents \n",
        "further forward in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8mRDHoAa5wUG"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MultiHeadSelfAttention(tf.keras.Model):\n",
        "    def __init__(self, model_dim, n_heads, dropout_rate=0.1):\n",
        "        '''\n",
        "        :param model_dim:       (int): Model dimension.\n",
        "        :param n_heads:         (int): Number of attention heads\n",
        "        :param dropout_rate:    (int): dropout_rate used in the Multiheaded attention layer\n",
        "        '''\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        assert model_dim % n_heads == 0, f'The dimension of the model must be divisible by the number of used attention heads.\\nargs:\\n\\tnumber of heads: {n_heads}\\n\\tmodel dimension: {model_dim}\\nrest:\\t´{model_dim%n_heads}'\n",
        "\n",
        "        self.model_dim = model_dim\n",
        "        self.n_heads = n_heads\n",
        "        # Calculating the head dimension\n",
        "        self.head_dim = int(model_dim/n_heads)\n",
        "        # Creating a key, query and value layer for all attention heads\n",
        "        self.key_layer = layers.Dense(self.head_dim, use_bias=False)\n",
        "        self.query_layer = layers.Dense(self.head_dim, use_bias=False)\n",
        "        self.value_layer = layers.Dense(self.head_dim, use_bias=False)\n",
        "        # Creating a dropout-layer for the linear layers\n",
        "        self.dropout = layers.Dropout(dropout_rate)\n",
        "        # Softmax function for the attention matrix\n",
        "        self.softmax = activation.Softmax(axis=3)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        '''\n",
        "        :param inputs: (tf.tensor): Input Tensor of shape [Batch_size, n_segments, model_dim]\n",
        "                                    If the multi-head attention object is part of the first transformer layer,\n",
        "                                    the input is the split and flattended batch of  images with both the width and\n",
        "                                    height embeddings allready added.\n",
        "\n",
        "        :return:       (tf.tensor). Output of the multi-head attention object. Shape [Batch_size, n_segments, model_dim]\n",
        "        '''\n",
        "        def create_mask(n_segments):\n",
        "            \"\"\"\n",
        "            This function creates a boolean mask for the attention-matrix.\n",
        "            This mask will be a simple mask which prevents forward attention.\n",
        "\n",
        "            Example for n_segments=4:\n",
        "\n",
        "            [[[[True], [False], [False], [False]],    \n",
        "              [[True], [True],  [False], [False]],\n",
        "              [[True], [True],  [True],  [False]],\n",
        "              [[True], [True],  [True],  [True]]\n",
        "            ]]\n",
        "\n",
        "            :param n_segments: (int): Number of image-segments of the input of the \n",
        "                               'call'-function.\n",
        "\n",
        "            :return:           (tf-tensor, dtype=bool): Attention maks of shape:[1, 1, n_segments, n_segments]\n",
        "            \"\"\"\n",
        "            mask = tf.linalg.band_part(tf.ones((1, 1, n_segments, n_segments), tf.bool), -1, 0)\n",
        "            return mask\n",
        "\n",
        "        # splitting the inputs into the correct number of n_heads_g\n",
        "        # Reshaping to [Batch_size, n_segments, n_heads, head_dimensions]\n",
        "        inputs = tf.reshape(inputs, [-1, tf.shape(inputs)[1], self.n_heads, self.head_dim])\n",
        "        keys = self.key_layer(self.dropout(inputs))\n",
        "        queries = self.query_layer(self.dropout(inputs))\n",
        "        values = self.value_layer(self.dropout(inputs))\n",
        "        # computing the attention-matrix\n",
        "        # Einsum Function: 'keys'[Batch_size, n_segments, n_heads, head_dimensions],\n",
        "        # 'queries'[Batch_size, n_segments, n_heads, head_dimensions] -> 'attention_-matrix'[Batch_size, n_heads,\n",
        "        # n_segments_queries, n_segments_keys]\n",
        "        attention_matrix = tf.einsum('nqhd, nkhd->nhqk', queries, keys)\n",
        "        # Scaling the attention matrix by the square-root of the model-dimension.\n",
        "        attention_matrix /= self.head_dim**0.5\n",
        "        # Creating the attention-mask.\n",
        "        attention_mask = create_mask(tf.shape(inputs)[1])\n",
        "        # Applying the softmax function to the attention matrix.\n",
        "        attention_matrix = self.softmax(attention_matrix, attention_mask)\n",
        "        # Using the scaled attention_matrix and applying it to the value tensor.\n",
        "        # Einsum Function: 'attention_matrix'[Batch_size, n_heads, n_segments_queries, n_segments_keys],\n",
        "        # 'values'[Batch_size, n_segments_values, n_heads, head_dimensions] -> 'output'[Batch_size, n_segments,\n",
        "        # n_heads, head_dimensions]\n",
        "        output = tf.einsum('nhql, nlhd->nqhd', attention_matrix, values)\n",
        "        #reshaping the output to [Batch_size, n_segments, model_dimensions]\n",
        "        output = tf.reshape(output, [-1, tf.shape(output)[1], self.model_dim])\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ElKcA4p5wXV",
        "outputId": "6a8b6e68-c990-4624-ca83-381eab554ae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean value of the output = -0.01421525701880455\n",
            "Shape of the output = (500, 48, 16)\n"
          ]
        }
      ],
      "source": [
        "# Testing and Debugging.\n",
        "\n",
        "# Creating the multiheadselfattention object.\n",
        "n_heads = 1\n",
        "model_dimension = 16\n",
        "multiheadselfattention = MultiHeadSelfAttention(model_dimension, n_heads)\n",
        "\n",
        "# Loading the input-images.\n",
        "test_images = test_batch[0]\n",
        "\n",
        "# Testing the model.\n",
        "out = multiheadselfattention(test_images)\n",
        "print(f'Mean value of the output = {tf.reduce_mean(out)}')\n",
        "print(f'Shape of the output = {out.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the MultiHeadAttention class, a transformer-layer class with linear expansion is created."
      ],
      "metadata": {
        "id": "zJm_RaqlG7CS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-WZUoqtJ5wsm"
      },
      "outputs": [],
      "source": [
        "class TransformerLayer(tf.keras.Model):\n",
        "    def __init__(self, model_dim, n_heads, linear_expansion, dropout_rate=0.1):\n",
        "        '''\n",
        "        :param model_dim:        (int): Dimensions of the model.\n",
        "        :param n_heads:          (int): Number of attention heads\n",
        "        :param linear_expansion: (int): During linear expansion, the output will be expanded to\n",
        "                                        linerar_expansion * model_dim\n",
        "        :param dropout_rate:     (int): dropout_rate used in the transformer-layer\n",
        "        '''\n",
        "        super(TransformerLayer, self).__init__()\n",
        "        self.model_dim = model_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.linear_expansion = linear_expansion\n",
        "        self.dropout_rate = dropout_rate\n",
        "        # defining the layers\n",
        "        self.multi_head_attention = MultiHeadSelfAttention(model_dim = self.model_dim,\n",
        "                                                           n_heads = self.n_heads,\n",
        "                                                           dropout_rate=self.dropout_rate)\n",
        "        self.input_layer = layers.InputLayer(input_shape=(self.model_dim, ))\n",
        "        self.layer_1 = layers.Dense(self.model_dim*self.linear_expansion, activation='relu')\n",
        "        self.layer_2 = layers.Dense(self.model_dim, activation='relu')\n",
        "        self.dropout = layers.Dropout(dropout_rate)\n",
        "        self.norm1 = layers.BatchNormalization()\n",
        "        self.norm2 = layers.BatchNormalization()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        '''\n",
        "        :param inputs: (tf.tensor): Input Tensor of shape [Batch_size, n_segments, model_dim]\n",
        "                                    If the multi-head attention object is part of the first transformer layer,\n",
        "                                    the input is the split and flattended batch of  images with both the width and\n",
        "                                    height embeddings allready added.\n",
        "        :return:       (tf.tensor). Output of the transformer-layer. Shape [Batch_size, n_segments, model_dim]\n",
        "        '''\n",
        "        skip_0 = inputs\n",
        "        out = self.multi_head_attention(inputs)\n",
        "        # Skip connection to counteract deminishing gradients\n",
        "        out = self.norm1(out + skip_0)\n",
        "        skip_1 = out\n",
        "        out = self.layer_1(self.dropout(out))\n",
        "        out = self.layer_2(self.dropout(out))\n",
        "        # Skip connection to counteract deminishing gradients\n",
        "        out = self.norm2(out + skip_1)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CFaJLpXonJF",
        "outputId": "003b5cd8-dbb0-4873-87c3-3351e6a9a119"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean value of the output = 0.21245361864566803\n",
            "Shape of the output = (500, 48, 16)\n"
          ]
        }
      ],
      "source": [
        "# Testing and Debugging.\n",
        "\n",
        "# Creating a transformerlayer object.\n",
        "n_heads = 1\n",
        "model_dimension = 16\n",
        "linear_expansion = 4\n",
        "transformerlayer = TransformerLayer(model_dimension, n_heads, linear_expansion)\n",
        "\n",
        "# Loading the input-images.\n",
        "test_images = test_batch[0]\n",
        "# Testing the model.\n",
        "out = transformerlayer(test_images)\n",
        "print(f'Mean value of the output = {tf.reduce_mean(out)}')\n",
        "print(f'Shape of the output = {out.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg8p44KFZg3K"
      },
      "source": [
        "The envisioned transformer model will transform each segment of the image individually.\n",
        "The model will then attend between the transformed tokens via multiheadattention.\n",
        "All these steps are not influenced by the order of the image segments in the sequence.\n",
        "This is unwanted for processing an image, because the order \n",
        "of the image-segments obviously matters a lot in a picture.\n",
        "For this reason to each image-semgent a vector is added, \n",
        "which encodes the position of this semgent in the context of the whole image.\n",
        "\n",
        "Positional Encoding as described in the paper \"Attention is all you need\"\n",
        "Doi: arXiv:1706.03762v5 [cs.CL] 6 Dec 2017"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PnPo9XdyeBAp"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PositionalEncoding():\n",
        "    '''\n",
        "    Creates an PositionalEncoding object, with defined maximum sequence length and encoding dimension.\n",
        "    '''\n",
        "    def __init__(self, max_length, encoding_dimension):\n",
        "        '''\n",
        "        :param max_length:           Maximum position that can be meening-fully encoded.\n",
        "\n",
        "        :param encoding_dimension:   Dimension of the encoded positions.\n",
        "                                     This parameter should match the encoding dimension of the transformer model, that is used.\n",
        "        '''\n",
        "        self.max_length = max_length                    # Highest encoded position.\n",
        "        self.encoding_dimension = encoding_dimension    # Output dimension of the positional encoding vector\n",
        "\n",
        "    def __call__(self, positions):\n",
        "        '''\n",
        "        :param positions: An array of positions to encode.\n",
        "                          All positions should be in form of ints.\n",
        "                          Example: positions=[0, 1, 3] would return a tensor of shape [3, encoding_dimension]\n",
        "                          with the encoded positions 0, 1, 3.\n",
        "        :return:          Tensorflow Tensor of shape [N_positons, encoding_dimension]\n",
        "        '''\n",
        "        encoded_positions = np.full([len(positions), self.encoding_dimension],  # 2D np.array of shape [N_positions, encoding_dimensions]\n",
        "                                    tf.range(self.encoding_dimension),          # filled with numbers from 0...(encoding_dimension-1)\n",
        "                                    dtype=np.float32)\n",
        "        encoded_positions = encoded_positions//2                                # all elements of array floor-devided by two\n",
        "        encoded_positions = self.max_length**(-2\n",
        "                                              *encoded_positions\n",
        "                                              /self.encoding_dimension)\n",
        "        positions_array = np.full([self.encoding_dimension,\n",
        "                                   len(positions)],\n",
        "                                  positions)\n",
        "\n",
        "        positions_array = np.swapaxes(positions_array, 0, 1)\n",
        "        encoded_positions = encoded_positions * positions_array\n",
        "        encoded_positions[::2] += np.pi/2                                       # Adding Pi/2 to every odd row, so that sin = cos for odd rows.\n",
        "        encoded_positions = np.sin(encoded_positions)                           # Calling the sinus function on each element in the array.\n",
        "        encoded_positions = tf.convert_to_tensor(encoded_positions,             # Converting output to tensorflow tensor.\n",
        "                                                 dtype=tf.float32)\n",
        "        return encoded_positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "ikLdDv-neQe4",
        "outputId": "9c2c02b0-9991-4fb9-dce6-87bd69f59bab"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x720 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Testing and Debugging\n",
        "# Shows the encoded-vectors for 100 successive positions (max_length = 100, encoding_dimension = 64)\n",
        "\n",
        "max_length = 100\n",
        "encoding_dimension = 64\n",
        "n_positions = 100\n",
        "\n",
        "# Generating the encodings.\n",
        "test_positional_encoder = PositionalEncoding(max_length, encoding_dimension)\n",
        "encoded_positions = test_positional_encoder([i for i in range(n_positions)])\n",
        "image_encoded_positions = tf.transpose(encoded_positions, [1, 0])\n",
        "\n",
        "# Plotting the Encodings.\n",
        "fig, axs = plt.subplots(1, figsize=[20, 10])\n",
        "im = axs.imshow(image_encoded_positions, cmap='gray')\n",
        "axs.set_xlabel('Position')\n",
        "axs.set_ylabel('Encoded Position as Vector')\n",
        "axs.set_title(f'Positional Encoding (Encoding Dimension = {encoding_dimension})')\n",
        "plt.colorbar(im)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "P6mpHk4bonVp"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(tf.keras.Model):\n",
        "    def __init__(self, model_dim, image_dim, n_heads, linear_expansion, n_layers, segment_width, dropout_rate=0.1, max_length=100):\n",
        "        '''\n",
        "        :param model_dim:        (int): Dimensions of the model.\n",
        "        :param image_dim:        (int): Number of pixels per image-segment.\n",
        "        :param linear_expansion: (int): During linear expansion in the transformer-layers,\n",
        "                                        the output will be expanded to linerar_expansion * model_dim\n",
        "        :param n_heads:          (int): Number of attention heads\n",
        "        :param n_layers:         (int): Number of layers the model will be comprised of.\n",
        "        :param segment_width     (int): Width of the segments the image will be split into.\n",
        "        :param dropout_rate:     (int): Dropout-rate used in the transformer-layer\n",
        "        '''\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.model_dim = model_dim\n",
        "        self.image_dim = image_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.linear_expansion = linear_expansion\n",
        "        self.n_layers = n_layers\n",
        "        self.segment_width = segment_width\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.max_length=max_length\n",
        "\n",
        "        # creating the positional encoder.\n",
        "        self.pos_enc = PositionalEncoding(self.max_length, self.model_dim)\n",
        "\n",
        "        self.norm1 = layers.BatchNormalization()\n",
        "        # Defining the layers\n",
        "        self.image_encoding = layers.Dense(self.model_dim)\n",
        "        self.transformer_layers = []\n",
        "        for _ in range(self.n_layers):\n",
        "            self.transformer_layers.append(TransformerLayer(self.model_dim,\n",
        "                                                self.n_heads,\n",
        "                                                self.linear_expansion,\n",
        "                                                self.dropout_rate))\n",
        "\n",
        "        self.norm_out = layers.BatchNormalization()\n",
        "        # The output of the model should be a RGB-image of the same size as the input.\n",
        "        self.output_layer = layers.Dense(self.image_dim, activation='sigmoid')\n",
        "        # Defining a learnable 'start_sequence_token' \n",
        "        self.start_sequence_token = tf.random.uniform([1, 1, self.model_dim])\n",
        "        \n",
        "\n",
        "    def call(self, inputs):\n",
        "        '''\n",
        "        :param inputs:  (tf.tensor): Batch of images\n",
        "        :return:       (tf.tensor): Batch of predicted segmentation masks. Shape=[batch_size, N_segments_height, N_segments_width]\n",
        "        '''\n",
        "        def add_start_token(inputs):\n",
        "            \"\"\"\n",
        "            This function adds the learnable 'start_sequence_token' to the start \n",
        "            of the image-segment seqences.           \n",
        "            :param inputs:  (tf.tensor): Input of the 'call'-function, flattened\n",
        "                                         by the 'flatten_batch' function.\n",
        "                                         shape=[batch_size, n_segments, N_pixels_per_segment]\n",
        "                                         \n",
        "            \"\"\"\n",
        "            batch_size = inputs.shape[0]\n",
        "            start_sequence_token = tf.concat([self.start_sequence_token for _ in range(batch_size)], axis=0)\n",
        "            inputs = tf.concat([start_sequence_token, inputs], axis=1)\n",
        "            return inputs\n",
        "\n",
        "        def add_positions(inputs, strength=0.4):\n",
        "            \"\"\"\n",
        "            This function encodes the positions of the image segments, after the \n",
        "            'flatten_batch' function.\n",
        "            :param inputs:   (tf.tensor): Inputs of the call function after flatten batch and image encoding call.\n",
        "            :param strength: (float): Factor by which the positional encodings are scaled before they are added to the image\n",
        "            \"\"\"\n",
        "            batch_size = inputs.shape[0]\n",
        "            positions = self.positional_encoding([i for i in range(inputs.shape[1])])\n",
        "            positions = tf.stack([positions for _ in range(batch_size)])\n",
        "            positions *= strength\n",
        "            return inputs + positions\n",
        "\n",
        "        skip_1 = self.image_encoding(inputs)\n",
        "        n_segments = tf.shape(skip_1)[1]\n",
        "        inputs = tf.numpy_function(add_start_token, [skip_1], tf.float32)\n",
        "        # See reshaping above. Soely for the graph.\n",
        "        inputs = tf.reshape(inputs, [-1, n_segments+1, self.model_dim])  # n_segments + 1 due to the added start-token\n",
        "\n",
        "        # Adding skip-connection to allow a gradient flow arround the numpy-functions.\n",
        "        slice_1 = inputs[:,1:] \n",
        "        slice_2 = inputs[:,:1]\n",
        "        slice_1 += skip_1\n",
        "        inputs = tf.concat([slice_2, slice_1], axis=1)\n",
        "        inputs = self.norm1(inputs)\n",
        "\n",
        "        for layer in self.transformer_layers:\n",
        "           inputs = layer(inputs)\n",
        "\n",
        "        inputs = self.norm_out(inputs)\n",
        "        inputs = self.output_layer(inputs)\n",
        "        output = inputs\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ankPZvYtonc9",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # Here the model is defined.\n",
        "model_dimension = 400 #@param {type:\"integer\"}\n",
        "segment_width = segment_width             # Must be identical to the segment widht used in the dataset!\n",
        "image_dim = segment_width**2\n",
        "n_heads = 4 #@param {type:\"integer\"}\n",
        "linear_expansion = 2 #@param {type:\"integer\"}\n",
        "n_layers = 3 #@param {type:\"integer\"}\n",
        "dropout_rate = 0.2 #@param {type:\"number\"}\n",
        "max_length = 100 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "model = TransformerModel(model_dimension,\n",
        "                         image_dim,\n",
        "                         n_heads,\n",
        "                         linear_expansion,\n",
        "                         n_layers,\n",
        "                         segment_width,\n",
        "                         dropout_rate, \n",
        "                         max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGvJ6ffaqzTO",
        "outputId": "11b76a25-38e2-44a9-f2c9-5e2ba7b9adb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 2s 16ms/step\n",
            "Shape of the output =  (500, 49, 16)\n",
            "Mean value of the output = 0.5117594003677368\n"
          ]
        }
      ],
      "source": [
        "# Testing and Debugging\n",
        "\n",
        "# Loading the input-images.\n",
        "out = model.predict(test_batch[0])\n",
        "print('Shape of the output = ', out.shape)\n",
        "print(f'Mean value of the output = {tf.reduce_mean(out)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0ypuNHoAWfjd"
      },
      "outputs": [],
      "source": [
        "# Defining the loss-function.\n",
        "loss_function = tf.keras.losses.MeanSquaredError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Mt0n8gdfWfvP"
      },
      "outputs": [],
      "source": [
        "# Compiling the model.\n",
        "model.compile(tf.keras.optimizers.Adam(learning_rate=1e-4), loss=loss_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF91LmAC-uKK",
        "outputId": "cb7b5702-32a3-4281-a23a-95b6b62890b6",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Loaded model from checkpoint\n"
          ]
        }
      ],
      "source": [
        "#@markdown Creating a custom callback, that is called after every epoch. This callback saves the model's weights if a new best val_loss is found.\n",
        "\n",
        "#@markdown Connecting the notebook to your google-drive!\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#@markdown Path to your model checkpoint dir:\n",
        "model_path = \"/content/drive/MyDrive/Colab Notebooks/Transformer_Image_Completion/model_checkpoints/Weights\" #@param {type:\"string\"}\n",
        "\n",
        "class CustomCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, model_path):\n",
        "        self.model_path = model_path\n",
        "        self.min_loss = 10**100\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_loss = logs['val_loss']\n",
        "        if val_loss < self.min_loss:\n",
        "            self.min_loss = val_loss\n",
        "            model.save_weights(self.model_path)\n",
        "\n",
        "my_callback = CustomCallback(model_path)\n",
        "\n",
        "#@markdown The saved model-checkpoint will be loaded, if the training was already started in a previous session.\n",
        "\n",
        "try:\n",
        "    model.load_weights(model_path)\n",
        "    print('Loaded model from checkpoint')\n",
        "except Exception: pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "isqiCVhLWfyO",
        "outputId": "2f2c1b5d-3348-4b28-9c10-bf477b10832b",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "120/120 [==============================] - 39s 296ms/step - loss: 0.0327 - val_loss: 0.0492\n",
            "Epoch 2/250\n",
            "120/120 [==============================] - 33s 276ms/step - loss: 0.0322 - val_loss: 0.0481\n",
            "Epoch 3/250\n",
            "120/120 [==============================] - 34s 283ms/step - loss: 0.0321 - val_loss: 0.0474\n",
            "Epoch 4/250\n",
            "120/120 [==============================] - 34s 280ms/step - loss: 0.0318 - val_loss: 0.0454\n",
            "Epoch 5/250\n",
            "120/120 [==============================] - 34s 279ms/step - loss: 0.0316 - val_loss: 0.0414\n",
            "Epoch 6/250\n",
            "120/120 [==============================] - 34s 280ms/step - loss: 0.0315 - val_loss: 0.0397\n",
            "Epoch 7/250\n",
            "120/120 [==============================] - 34s 280ms/step - loss: 0.0313 - val_loss: 0.0393\n",
            "Epoch 8/250\n",
            "120/120 [==============================] - 34s 280ms/step - loss: 0.0312 - val_loss: 0.0385\n",
            "Epoch 9/250\n",
            "120/120 [==============================] - 34s 280ms/step - loss: 0.0312 - val_loss: 0.0377\n",
            "Epoch 10/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0311 - val_loss: 0.0390\n",
            "Epoch 11/250\n",
            "120/120 [==============================] - 33s 279ms/step - loss: 0.0309 - val_loss: 0.0393\n",
            "Epoch 12/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0307 - val_loss: 0.0387\n",
            "Epoch 13/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0307 - val_loss: 0.0387\n",
            "Epoch 14/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0305 - val_loss: 0.0382\n",
            "Epoch 15/250\n",
            "120/120 [==============================] - 34s 280ms/step - loss: 0.0304 - val_loss: 0.0370\n",
            "Epoch 16/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0303 - val_loss: 0.0373\n",
            "Epoch 17/250\n",
            "120/120 [==============================] - 34s 280ms/step - loss: 0.0304 - val_loss: 0.0359\n",
            "Epoch 18/250\n",
            "120/120 [==============================] - 33s 279ms/step - loss: 0.0301 - val_loss: 0.0367\n",
            "Epoch 19/250\n",
            "120/120 [==============================] - 33s 279ms/step - loss: 0.0301 - val_loss: 0.0382\n",
            "Epoch 20/250\n",
            "120/120 [==============================] - 34s 279ms/step - loss: 0.0300 - val_loss: 0.0356\n",
            "Epoch 21/250\n",
            "120/120 [==============================] - 34s 280ms/step - loss: 0.0300 - val_loss: 0.0348\n",
            "Epoch 22/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0300 - val_loss: 0.0369\n",
            "Epoch 23/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0297 - val_loss: 0.0369\n",
            "Epoch 24/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0296 - val_loss: 0.0364\n",
            "Epoch 25/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0296 - val_loss: 0.0370\n",
            "Epoch 26/250\n",
            "120/120 [==============================] - 34s 279ms/step - loss: 0.0294 - val_loss: 0.0347\n",
            "Epoch 27/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0294 - val_loss: 0.0377\n",
            "Epoch 28/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0293 - val_loss: 0.0369\n",
            "Epoch 29/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0292 - val_loss: 0.0365\n",
            "Epoch 30/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0291 - val_loss: 0.0364\n",
            "Epoch 31/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0290 - val_loss: 0.0374\n",
            "Epoch 32/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0290 - val_loss: 0.0377\n",
            "Epoch 33/250\n",
            "120/120 [==============================] - 33s 279ms/step - loss: 0.0289 - val_loss: 0.0381\n",
            "Epoch 34/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0288 - val_loss: 0.0383\n",
            "Epoch 35/250\n",
            "120/120 [==============================] - 33s 279ms/step - loss: 0.0287 - val_loss: 0.0380\n",
            "Epoch 36/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0287 - val_loss: 0.0386\n",
            "Epoch 37/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0287 - val_loss: 0.0373\n",
            "Epoch 38/250\n",
            "120/120 [==============================] - 33s 279ms/step - loss: 0.0285 - val_loss: 0.0389\n",
            "Epoch 39/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0285 - val_loss: 0.0387\n",
            "Epoch 40/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0284 - val_loss: 0.0382\n",
            "Epoch 41/250\n",
            "120/120 [==============================] - 33s 279ms/step - loss: 0.0284 - val_loss: 0.0379\n",
            "Epoch 42/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0283 - val_loss: 0.0381\n",
            "Epoch 43/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0282 - val_loss: 0.0390\n",
            "Epoch 44/250\n",
            "120/120 [==============================] - 34s 279ms/step - loss: 0.0281 - val_loss: 0.0389\n",
            "Epoch 45/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0282 - val_loss: 0.0388\n",
            "Epoch 46/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0281 - val_loss: 0.0401\n",
            "Epoch 47/250\n",
            "120/120 [==============================] - 33s 279ms/step - loss: 0.0280 - val_loss: 0.0385\n",
            "Epoch 48/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0279 - val_loss: 0.0399\n",
            "Epoch 49/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0280 - val_loss: 0.0370\n",
            "Epoch 50/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0279 - val_loss: 0.0358\n",
            "Epoch 51/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0278 - val_loss: 0.0374\n",
            "Epoch 52/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0277 - val_loss: 0.0382\n",
            "Epoch 53/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0277 - val_loss: 0.0382\n",
            "Epoch 54/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0276 - val_loss: 0.0391\n",
            "Epoch 55/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0275 - val_loss: 0.0396\n",
            "Epoch 56/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0274 - val_loss: 0.0393\n",
            "Epoch 57/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0274 - val_loss: 0.0397\n",
            "Epoch 58/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0273 - val_loss: 0.0400\n",
            "Epoch 59/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0273 - val_loss: 0.0402\n",
            "Epoch 60/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0272 - val_loss: 0.0402\n",
            "Epoch 61/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0272 - val_loss: 0.0402\n",
            "Epoch 62/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0271 - val_loss: 0.0381\n",
            "Epoch 63/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0271 - val_loss: 0.0409\n",
            "Epoch 64/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0270 - val_loss: 0.0393\n",
            "Epoch 65/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0270 - val_loss: 0.0388\n",
            "Epoch 66/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0269 - val_loss: 0.0390\n",
            "Epoch 67/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0269 - val_loss: 0.0406\n",
            "Epoch 68/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0268 - val_loss: 0.0406\n",
            "Epoch 69/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0267 - val_loss: 0.0394\n",
            "Epoch 70/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0267 - val_loss: 0.0412\n",
            "Epoch 71/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0267 - val_loss: 0.0392\n",
            "Epoch 72/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0266 - val_loss: 0.0405\n",
            "Epoch 73/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0266 - val_loss: 0.0392\n",
            "Epoch 74/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0266 - val_loss: 0.0407\n",
            "Epoch 75/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0264 - val_loss: 0.0399\n",
            "Epoch 76/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0264 - val_loss: 0.0419\n",
            "Epoch 77/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0264 - val_loss: 0.0423\n",
            "Epoch 78/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0263 - val_loss: 0.0400\n",
            "Epoch 79/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0263 - val_loss: 0.0412\n",
            "Epoch 80/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0263 - val_loss: 0.0412\n",
            "Epoch 81/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0262 - val_loss: 0.0400\n",
            "Epoch 82/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0261 - val_loss: 0.0400\n",
            "Epoch 83/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0261 - val_loss: 0.0403\n",
            "Epoch 84/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0261 - val_loss: 0.0410\n",
            "Epoch 85/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0260 - val_loss: 0.0413\n",
            "Epoch 86/250\n",
            "120/120 [==============================] - 33s 279ms/step - loss: 0.0260 - val_loss: 0.0406\n",
            "Epoch 87/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0260 - val_loss: 0.0426\n",
            "Epoch 88/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0259 - val_loss: 0.0409\n",
            "Epoch 89/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0259 - val_loss: 0.0426\n",
            "Epoch 90/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0258 - val_loss: 0.0399\n",
            "Epoch 91/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0258 - val_loss: 0.0411\n",
            "Epoch 92/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0257 - val_loss: 0.0417\n",
            "Epoch 93/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0257 - val_loss: 0.0419\n",
            "Epoch 94/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0257 - val_loss: 0.0412\n",
            "Epoch 95/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0256 - val_loss: 0.0414\n",
            "Epoch 96/250\n",
            "120/120 [==============================] - 33s 279ms/step - loss: 0.0256 - val_loss: 0.0422\n",
            "Epoch 97/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0256 - val_loss: 0.0419\n",
            "Epoch 98/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0255 - val_loss: 0.0396\n",
            "Epoch 99/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0254 - val_loss: 0.0417\n",
            "Epoch 100/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0254 - val_loss: 0.0428\n",
            "Epoch 101/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0254 - val_loss: 0.0415\n",
            "Epoch 102/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0253 - val_loss: 0.0431\n",
            "Epoch 103/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0253 - val_loss: 0.0426\n",
            "Epoch 104/250\n",
            "120/120 [==============================] - 33s 279ms/step - loss: 0.0253 - val_loss: 0.0429\n",
            "Epoch 105/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0252 - val_loss: 0.0434\n",
            "Epoch 106/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0252 - val_loss: 0.0429\n",
            "Epoch 107/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0252 - val_loss: 0.0420\n",
            "Epoch 108/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0251 - val_loss: 0.0422\n",
            "Epoch 109/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0251 - val_loss: 0.0404\n",
            "Epoch 110/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0250 - val_loss: 0.0405\n",
            "Epoch 111/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0250 - val_loss: 0.0424\n",
            "Epoch 112/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0250 - val_loss: 0.0413\n",
            "Epoch 113/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0249 - val_loss: 0.0420\n",
            "Epoch 114/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0249 - val_loss: 0.0423\n",
            "Epoch 115/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0249 - val_loss: 0.0431\n",
            "Epoch 116/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0248 - val_loss: 0.0423\n",
            "Epoch 117/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0248 - val_loss: 0.0414\n",
            "Epoch 118/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0247 - val_loss: 0.0435\n",
            "Epoch 119/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0247 - val_loss: 0.0420\n",
            "Epoch 120/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0247 - val_loss: 0.0417\n",
            "Epoch 121/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0247 - val_loss: 0.0433\n",
            "Epoch 122/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0246 - val_loss: 0.0432\n",
            "Epoch 123/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0246 - val_loss: 0.0432\n",
            "Epoch 124/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0246 - val_loss: 0.0420\n",
            "Epoch 125/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0245 - val_loss: 0.0430\n",
            "Epoch 126/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0245 - val_loss: 0.0421\n",
            "Epoch 127/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0245 - val_loss: 0.0425\n",
            "Epoch 128/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0244 - val_loss: 0.0419\n",
            "Epoch 129/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0244 - val_loss: 0.0428\n",
            "Epoch 130/250\n",
            "120/120 [==============================] - 33s 279ms/step - loss: 0.0244 - val_loss: 0.0437\n",
            "Epoch 131/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0244 - val_loss: 0.0431\n",
            "Epoch 132/250\n",
            "120/120 [==============================] - 33s 279ms/step - loss: 0.0243 - val_loss: 0.0427\n",
            "Epoch 133/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0243 - val_loss: 0.0441\n",
            "Epoch 134/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0243 - val_loss: 0.0420\n",
            "Epoch 135/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0243 - val_loss: 0.0438\n",
            "Epoch 136/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0242 - val_loss: 0.0433\n",
            "Epoch 137/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0242 - val_loss: 0.0424\n",
            "Epoch 138/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0242 - val_loss: 0.0441\n",
            "Epoch 139/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0242 - val_loss: 0.0428\n",
            "Epoch 140/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0241 - val_loss: 0.0431\n",
            "Epoch 141/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0241 - val_loss: 0.0432\n",
            "Epoch 142/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0241 - val_loss: 0.0433\n",
            "Epoch 143/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0240 - val_loss: 0.0436\n",
            "Epoch 144/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0240 - val_loss: 0.0438\n",
            "Epoch 145/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0240 - val_loss: 0.0446\n",
            "Epoch 146/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0239 - val_loss: 0.0441\n",
            "Epoch 147/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0239 - val_loss: 0.0457\n",
            "Epoch 148/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0239 - val_loss: 0.0454\n",
            "Epoch 149/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0239 - val_loss: 0.0467\n",
            "Epoch 150/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0238 - val_loss: 0.0457\n",
            "Epoch 151/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0238 - val_loss: 0.0457\n",
            "Epoch 152/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0238 - val_loss: 0.0445\n",
            "Epoch 153/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0238 - val_loss: 0.0449\n",
            "Epoch 154/250\n",
            "120/120 [==============================] - 33s 276ms/step - loss: 0.0237 - val_loss: 0.0446\n",
            "Epoch 155/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0237 - val_loss: 0.0444\n",
            "Epoch 156/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0237 - val_loss: 0.0444\n",
            "Epoch 157/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0237 - val_loss: 0.0450\n",
            "Epoch 158/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0236 - val_loss: 0.0460\n",
            "Epoch 159/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0236 - val_loss: 0.0465\n",
            "Epoch 160/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0236 - val_loss: 0.0457\n",
            "Epoch 161/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0236 - val_loss: 0.0467\n",
            "Epoch 162/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0235 - val_loss: 0.0461\n",
            "Epoch 163/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0235 - val_loss: 0.0452\n",
            "Epoch 164/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0235 - val_loss: 0.0462\n",
            "Epoch 165/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0235 - val_loss: 0.0463\n",
            "Epoch 166/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0234 - val_loss: 0.0455\n",
            "Epoch 167/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0234 - val_loss: 0.0462\n",
            "Epoch 168/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0234 - val_loss: 0.0457\n",
            "Epoch 169/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0234 - val_loss: 0.0473\n",
            "Epoch 170/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0234 - val_loss: 0.0463\n",
            "Epoch 171/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0233 - val_loss: 0.0470\n",
            "Epoch 172/250\n",
            "120/120 [==============================] - 33s 279ms/step - loss: 0.0233 - val_loss: 0.0471\n",
            "Epoch 173/250\n",
            "120/120 [==============================] - 33s 279ms/step - loss: 0.0233 - val_loss: 0.0464\n",
            "Epoch 174/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0233 - val_loss: 0.0458\n",
            "Epoch 175/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0232 - val_loss: 0.0463\n",
            "Epoch 176/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0232 - val_loss: 0.0472\n",
            "Epoch 177/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0232 - val_loss: 0.0463\n",
            "Epoch 178/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0232 - val_loss: 0.0437\n",
            "Epoch 179/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0232 - val_loss: 0.0448\n",
            "Epoch 180/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0232 - val_loss: 0.0467\n",
            "Epoch 181/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0231 - val_loss: 0.0479\n",
            "Epoch 182/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0231 - val_loss: 0.0476\n",
            "Epoch 183/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0231 - val_loss: 0.0467\n",
            "Epoch 184/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0230 - val_loss: 0.0471\n",
            "Epoch 185/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0230 - val_loss: 0.0478\n",
            "Epoch 186/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0230 - val_loss: 0.0460\n",
            "Epoch 187/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0230 - val_loss: 0.0470\n",
            "Epoch 188/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0230 - val_loss: 0.0469\n",
            "Epoch 189/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0230 - val_loss: 0.0467\n",
            "Epoch 190/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0229 - val_loss: 0.0464\n",
            "Epoch 191/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0229 - val_loss: 0.0480\n",
            "Epoch 192/250\n",
            "120/120 [==============================] - 33s 279ms/step - loss: 0.0229 - val_loss: 0.0473\n",
            "Epoch 193/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0229 - val_loss: 0.0472\n",
            "Epoch 194/250\n",
            "120/120 [==============================] - 33s 279ms/step - loss: 0.0229 - val_loss: 0.0479\n",
            "Epoch 195/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0228 - val_loss: 0.0471\n",
            "Epoch 196/250\n",
            "120/120 [==============================] - 34s 279ms/step - loss: 0.0228 - val_loss: 0.0466\n",
            "Epoch 197/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0228 - val_loss: 0.0467\n",
            "Epoch 198/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0228 - val_loss: 0.0464\n",
            "Epoch 199/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0228 - val_loss: 0.0472\n",
            "Epoch 200/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0227 - val_loss: 0.0473\n",
            "Epoch 201/250\n",
            "120/120 [==============================] - 34s 279ms/step - loss: 0.0227 - val_loss: 0.0482\n",
            "Epoch 202/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0227 - val_loss: 0.0468\n",
            "Epoch 203/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0227 - val_loss: 0.0474\n",
            "Epoch 204/250\n",
            "120/120 [==============================] - 33s 279ms/step - loss: 0.0227 - val_loss: 0.0485\n",
            "Epoch 205/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0227 - val_loss: 0.0477\n",
            "Epoch 206/250\n",
            "120/120 [==============================] - 34s 279ms/step - loss: 0.0227 - val_loss: 0.0474\n",
            "Epoch 207/250\n",
            "120/120 [==============================] - 33s 279ms/step - loss: 0.0226 - val_loss: 0.0480\n",
            "Epoch 208/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0226 - val_loss: 0.0476\n",
            "Epoch 209/250\n",
            "120/120 [==============================] - 34s 279ms/step - loss: 0.0226 - val_loss: 0.0473\n",
            "Epoch 210/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0225 - val_loss: 0.0475\n",
            "Epoch 211/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0226 - val_loss: 0.0476\n",
            "Epoch 212/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0225 - val_loss: 0.0483\n",
            "Epoch 213/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0225 - val_loss: 0.0472\n",
            "Epoch 214/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0225 - val_loss: 0.0480\n",
            "Epoch 215/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0225 - val_loss: 0.0474\n",
            "Epoch 216/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0225 - val_loss: 0.0481\n",
            "Epoch 217/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0224 - val_loss: 0.0482\n",
            "Epoch 218/250\n",
            "120/120 [==============================] - 33s 279ms/step - loss: 0.0225 - val_loss: 0.0471\n",
            "Epoch 219/250\n",
            "120/120 [==============================] - 33s 277ms/step - loss: 0.0224 - val_loss: 0.0477\n",
            "Epoch 220/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0224 - val_loss: 0.0480\n",
            "Epoch 221/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0224 - val_loss: 0.0472\n",
            "Epoch 222/250\n",
            "120/120 [==============================] - 33s 278ms/step - loss: 0.0223 - val_loss: 0.0472\n",
            "Epoch 223/250\n",
            " 94/120 [======================>.......] - ETA: 6s - loss: 0.0224"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-d3bdae027a18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@markdown # Training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m250\u001b[0m \u001b[0;31m#@param {type:\"integer\"}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds_processed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_ds_processed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmy_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@markdown # Training the model\n",
        "epochs = 250 #@param {type:\"integer\"}\n",
        "history = model.fit(train_ds_processed, validation_data=test_ds_processed, epochs=epochs, callbacks=[my_callback])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a50f2ec9b89c402689944324118e811e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6cb2c3cfdcac490f922130c08c7ab0e4",
              "IPY_MODEL_fc227b4eb2c34a29977e01452218d0e9",
              "IPY_MODEL_717fa8ef14c247c48d3f8f7cdc59192a"
            ],
            "layout": "IPY_MODEL_59f43cd43cb7482e9c218aae94e64388"
          }
        },
        "6cb2c3cfdcac490f922130c08c7ab0e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_802104c207e34202927da2a56e4a66c5",
            "placeholder": "​",
            "style": "IPY_MODEL_d00ead86656c416e983b7908bdd37cc2",
            "value": "Dl Completed...: 100%"
          }
        },
        "fc227b4eb2c34a29977e01452218d0e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2848dca4e71e45a4884facb0021c2058",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad79248fc11e4c439026e4591030a44d",
            "value": 5
          }
        },
        "717fa8ef14c247c48d3f8f7cdc59192a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc2f769afa1f4378b6dc60a2d70c7765",
            "placeholder": "​",
            "style": "IPY_MODEL_3cf0096cd0e2455fb8d521ca059747c6",
            "value": " 5/5 [00:00&lt;00:00, 12.37 file/s]"
          }
        },
        "59f43cd43cb7482e9c218aae94e64388": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "802104c207e34202927da2a56e4a66c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d00ead86656c416e983b7908bdd37cc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2848dca4e71e45a4884facb0021c2058": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad79248fc11e4c439026e4591030a44d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc2f769afa1f4378b6dc60a2d70c7765": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cf0096cd0e2455fb8d521ca059747c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}