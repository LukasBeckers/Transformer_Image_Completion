{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1806126824646298753\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 2962371380\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 619049534612755935\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Checking if a GPU is available on the system.\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\c8h10\\tensorflow_datasets\\imagenet_r\\0.2.0...\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "Dl Completed...: 0 url [00:00, ? url/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2ec018b3138e46e0a61446949ea1258f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Dl Size...: 0 MiB [00:00, ? MiB/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0d57176830aa4b89968b0cab883c8d24"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extraction completed...: 0 file [00:00, ? file/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d38cabf68f9f4c74a42829711d93d755"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0069be8d21b1472086421197967b2ccf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test examples...: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f08a2d50fa2d4bf183bd555af8f5d3b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe4 in position 194: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Loading the Imagenet_r dataset.\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m imagenet_r \u001B[38;5;241m=\u001B[39m \u001B[43mtfds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimagenet_r\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:250\u001B[0m, in \u001B[0;36mload.<locals>.decorator\u001B[1;34m(function, unused_none_instance, args, kwargs)\u001B[0m\n\u001B[0;32m    248\u001B[0m name \u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m args \u001B[38;5;28;01melse\u001B[39;00m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 250\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m function(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    251\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    252\u001B[0m   metadata\u001B[38;5;241m.\u001B[39mmark_error()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_datasets\\core\\load.py:575\u001B[0m, in \u001B[0;36mload\u001B[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001B[0m\n\u001B[0;32m    573\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m download:\n\u001B[0;32m    574\u001B[0m   download_and_prepare_kwargs \u001B[38;5;241m=\u001B[39m download_and_prepare_kwargs \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[1;32m--> 575\u001B[0m   dbuilder\u001B[38;5;241m.\u001B[39mdownload_and_prepare(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdownload_and_prepare_kwargs)\n\u001B[0;32m    577\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m as_dataset_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    578\u001B[0m   as_dataset_kwargs \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:523\u001B[0m, in \u001B[0;36mDatasetBuilder.download_and_prepare\u001B[1;34m(self, download_dir, download_config, file_format)\u001B[0m\n\u001B[0;32m    521\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mread_from_directory(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_dir)\n\u001B[0;32m    522\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 523\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_download_and_prepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    524\u001B[0m \u001B[43m      \u001B[49m\u001B[43mdl_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdl_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    525\u001B[0m \u001B[43m      \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    526\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    528\u001B[0m   \u001B[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001B[39;00m\n\u001B[0;32m    529\u001B[0m   \u001B[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001B[39;00m\n\u001B[0;32m    530\u001B[0m   \u001B[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001B[39;00m\n\u001B[0;32m    531\u001B[0m   \u001B[38;5;66;03m# when reading from package data.\u001B[39;00m\n\u001B[0;32m    532\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdownload_size \u001B[38;5;241m=\u001B[39m dl_manager\u001B[38;5;241m.\u001B[39mdownloaded_size\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1278\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder._download_and_prepare\u001B[1;34m(self, dl_manager, download_config)\u001B[0m\n\u001B[0;32m   1267\u001B[0m   \u001B[38;5;28;01mfor\u001B[39;00m split_name, generator \u001B[38;5;129;01min\u001B[39;00m utils\u001B[38;5;241m.\u001B[39mtqdm(\n\u001B[0;32m   1268\u001B[0m       split_generators\u001B[38;5;241m.\u001B[39mitems(),\n\u001B[0;32m   1269\u001B[0m       desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenerating splits...\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1270\u001B[0m       unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m splits\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1271\u001B[0m       leave\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m   1272\u001B[0m   ):\n\u001B[0;32m   1273\u001B[0m     filename_template \u001B[38;5;241m=\u001B[39m naming\u001B[38;5;241m.\u001B[39mShardedFileTemplate(\n\u001B[0;32m   1274\u001B[0m         split\u001B[38;5;241m=\u001B[39msplit_name,\n\u001B[0;32m   1275\u001B[0m         dataset_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname,\n\u001B[0;32m   1276\u001B[0m         data_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_path,\n\u001B[0;32m   1277\u001B[0m         filetype_suffix\u001B[38;5;241m=\u001B[39mpath_suffix)\n\u001B[1;32m-> 1278\u001B[0m     future \u001B[38;5;241m=\u001B[39m \u001B[43msplit_builder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubmit_split_generation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1279\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msplit_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1280\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgenerator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgenerator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1281\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename_template\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilename_template\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1282\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisable_shuffling\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minfo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisable_shuffling\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1283\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1284\u001B[0m     split_info_futures\u001B[38;5;241m.\u001B[39mappend(future)\n\u001B[0;32m   1286\u001B[0m \u001B[38;5;66;03m# Process the result of the beam pipeline.\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_datasets\\core\\split_builder.py:311\u001B[0m, in \u001B[0;36mSplitBuilder.submit_split_generation\u001B[1;34m(self, split_name, generator, filename_template, disable_shuffling)\u001B[0m\n\u001B[0;32m    308\u001B[0m \u001B[38;5;66;03m# Depending on the type of generator, we use the corresponding\u001B[39;00m\n\u001B[0;32m    309\u001B[0m \u001B[38;5;66;03m# `_build_from_xyz` method.\u001B[39;00m\n\u001B[0;32m    310\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(generator, collections\u001B[38;5;241m.\u001B[39mabc\u001B[38;5;241m.\u001B[39mIterable):\n\u001B[1;32m--> 311\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_from_generator(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mbuild_kwargs)\n\u001B[0;32m    312\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# Otherwise, beam required\u001B[39;00m\n\u001B[0;32m    313\u001B[0m   unknown_generator_type \u001B[38;5;241m=\u001B[39m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m    314\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInvalid split generator value for split `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msplit_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`. \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    315\u001B[0m       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mExpected generator or apache_beam object. Got: \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    316\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(generator)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_datasets\\core\\split_builder.py:384\u001B[0m, in \u001B[0;36mSplitBuilder._build_from_generator\u001B[1;34m(self, split_name, generator, filename_template, disable_shuffling)\u001B[0m\n\u001B[0;32m    382\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m    383\u001B[0m     utils\u001B[38;5;241m.\u001B[39mreraise(e, prefix\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFailed to encode example:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mexample\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 384\u001B[0m   \u001B[43mwriter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexample\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    385\u001B[0m shard_lengths, total_size \u001B[38;5;241m=\u001B[39m writer\u001B[38;5;241m.\u001B[39mfinalize()\n\u001B[0;32m    387\u001B[0m split_info \u001B[38;5;241m=\u001B[39m splits_lib\u001B[38;5;241m.\u001B[39mSplitInfo(\n\u001B[0;32m    388\u001B[0m     name\u001B[38;5;241m=\u001B[39msplit_name,\n\u001B[0;32m    389\u001B[0m     shard_lengths\u001B[38;5;241m=\u001B[39mshard_lengths,\n\u001B[0;32m    390\u001B[0m     num_bytes\u001B[38;5;241m=\u001B[39mtotal_size,\n\u001B[0;32m    391\u001B[0m     filename_template\u001B[38;5;241m=\u001B[39mfilename_template,\n\u001B[0;32m    392\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_datasets\\core\\writer.py:277\u001B[0m, in \u001B[0;36mWriter.write\u001B[1;34m(self, key, example)\u001B[0m\n\u001B[0;32m    266\u001B[0m \u001B[38;5;124;03m\"\"\"Writes given Example.\u001B[39;00m\n\u001B[0;32m    267\u001B[0m \n\u001B[0;32m    268\u001B[0m \u001B[38;5;124;03mThe given example is not directly written to the tfrecord file, but to a\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    274\u001B[0m \u001B[38;5;124;03m  example: the Example to write to the tfrecord file.\u001B[39;00m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    276\u001B[0m serialized_example \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_serializer\u001B[38;5;241m.\u001B[39mserialize_example(example\u001B[38;5;241m=\u001B[39mexample)\n\u001B[1;32m--> 277\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_shuffler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mserialized_example\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    278\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_examples \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_datasets\\core\\shuffle.py:234\u001B[0m, in \u001B[0;36mShuffler.add\u001B[1;34m(self, key, data)\u001B[0m\n\u001B[0;32m    232\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_total_bytes \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(data)\n\u001B[0;32m    233\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_in_memory:\n\u001B[1;32m--> 234\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_add_to_mem_buffer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    236\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_add_to_bucket(hkey, data)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_datasets\\core\\shuffle.py:217\u001B[0m, in \u001B[0;36mShuffler._add_to_mem_buffer\u001B[1;34m(self, hkey, data)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_total_bytes \u001B[38;5;241m>\u001B[39m MAX_MEM_BUFFER_SIZE:\n\u001B[0;32m    216\u001B[0m   \u001B[38;5;28;01mfor\u001B[39;00m hkey, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mem_buffer:\n\u001B[1;32m--> 217\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_add_to_bucket\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    218\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mem_buffer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    219\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_in_memory \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_datasets\\core\\shuffle.py:211\u001B[0m, in \u001B[0;36mShuffler._add_to_bucket\u001B[1;34m(self, hkey, data)\u001B[0m\n\u001B[0;32m    209\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_add_to_bucket\u001B[39m(\u001B[38;5;28mself\u001B[39m, hkey, data):\n\u001B[0;32m    210\u001B[0m   bucket_number \u001B[38;5;241m=\u001B[39m get_bucket_number(hkey\u001B[38;5;241m=\u001B[39mhkey, num_buckets\u001B[38;5;241m=\u001B[39mBUCKETS_NUMBER)\n\u001B[1;32m--> 211\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_buckets\u001B[49m\u001B[43m[\u001B[49m\u001B[43mbucket_number\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_datasets\\core\\shuffle.py:139\u001B[0m, in \u001B[0;36m_Bucket.add\u001B[1;34m(self, key, data)\u001B[0m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;66;03m# http://docs.python.org/3/library/struct.html#byte-order-size-and-alignment\u001B[39;00m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;66;03m# The equal sign (\"=\") is important here, has it guarantees the standard\u001B[39;00m\n\u001B[0;32m    133\u001B[0m \u001B[38;5;66;03m# size (Q: 8 bytes) is used, as opposed to native size, which can differ\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;66;03m# We do not specify endianess (platform dependent), but this is OK since the\u001B[39;00m\n\u001B[0;32m    137\u001B[0m \u001B[38;5;66;03m# temporary files are going to be written and read by the same platform.\u001B[39;00m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fobj\u001B[38;5;241m.\u001B[39mwrite(struct\u001B[38;5;241m.\u001B[39mpack(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m=Q\u001B[39m\u001B[38;5;124m'\u001B[39m, data_size))\n\u001B[1;32m--> 139\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_length \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_size \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m data_size\n",
      "File \u001B[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:100\u001B[0m, in \u001B[0;36mFileIO.write\u001B[1;34m(self, file_content)\u001B[0m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;124;03m\"\"\"Writes file_content to the file. Appends to the end of the file.\"\"\"\u001B[39;00m\n\u001B[0;32m     99\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prewrite_check()\n\u001B[1;32m--> 100\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_writable_file\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcompat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_bytes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_content\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'utf-8' codec can't decode byte 0xe4 in position 194: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "# Loading the Imagenet_r dataset.\n",
    "\n",
    "imagenet_r = tfds.load('imagenet_r')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}